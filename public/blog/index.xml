<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Yanfei Wu</title>
    <link>https://yanfei-wu.github.io/blog/index.xml</link>
    <description>Recent content in Blogs on Yanfei Wu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2016 Yanfei Wu</copyright>
    <lastBuildDate>Sat, 19 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://yanfei-wu.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>S&amp;P 500 Stocks Analysis</title>
      <link>https://yanfei-wu.github.io/blog/2017-08-19_stock_analysis/</link>
      <pubDate>Sat, 19 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yanfei-wu.github.io/blog/2017-08-19_stock_analysis/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The code for this project is available in &lt;a href=&#34;https://github.com/yanfei-wu/stock/&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/S%26P_500_Index&#34;&gt;S&amp;amp;P 500&lt;/a&gt; is an American stock market index based on the market capitalizations of &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies&#34;&gt;505 large companies&lt;/a&gt; having common stock listed on the NYSE or NASDAQ. It covers about 80 percent of the American equity market by capitalization. The index is weighted by free-float market capitalization, so the more valuable a company is, the relatively more of the index it accounts for.&lt;/p&gt;

&lt;p&gt;This project focuses on handling and analyzing stock market data with Python. The historical stock data over the past 5 years is obtained from Google Finance and imported into Python Pandas. The data is manipulated to calculate a variety of performance metrics including returns, volatility, alpha, beta, Sharpe ratio, and stock moving averages. The different stocks as well as sectors contained in the S&amp;amp;P 500 index are compared based on these metrics. The best and worst performing stocks/sectors are identified.&lt;/p&gt;

&lt;p&gt;Note that the ultimate goal for stock analysis is to design an optimal portforlio and a trading model that can potentially allow maximized returns, but this is beyond the scope of this post and will be explored in a future post.&lt;/p&gt;

&lt;h2 id=&#34;2-analysis&#34;&gt;2. Analysis&lt;/h2&gt;

&lt;h3 id=&#34;2-1-data&#34;&gt;2.1 Data&lt;/h3&gt;

&lt;p&gt;The 5-year historical stock price data for all individual stocks contained in the S&amp;amp;P 500 index is obtained from &lt;a href=&#34;https://finance.google.com/finance&#34;&gt;Google Finance&lt;/a&gt;. Note that there are many other sources to get stock data, such as &lt;a href=&#34;https://finance.yahoo.com&#34;&gt;Yahoo! Finance&lt;/a&gt;, &lt;a href=&#34;https://blog.quandl.com/api-for-stock-data&#34;&gt;Quandl API&lt;/a&gt;. The price data for individual stocks is then concatenated into one master dataset. The price data of SPDR S&amp;amp;P 500 trust ETF (SPY, designed to track the S&amp;amp;P 500 stock market index) is also obtained and added into the dataset to sereve as reference. The information of Global Industry Classification Standard (GICS) sector of each stock is included in the the &lt;code&gt;Sector&lt;/code&gt; attribute of the dataset. The final dataset contains the following fields:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Date&lt;/code&gt;: date in the format of YYYY-MM-DD&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Open&lt;/code&gt;: the stock open price&lt;/li&gt;
&lt;li&gt;&lt;code&gt;High&lt;/code&gt;: the highest price of the day&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Low&lt;/code&gt;: the lowest price of the day&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Close&lt;/code&gt;: the stock close price&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Volume&lt;/code&gt;: the stock trading volume&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Name&lt;/code&gt;: the stock ticker symbol&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Sector&lt;/code&gt;: the GICS sector name&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After reading the dataset into Pandas, it is found that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The dataset contains 605,586 rows and 8 columns.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;The fields of &lt;code&gt;Open&lt;/code&gt;, &lt;code&gt;High&lt;/code&gt;, &lt;code&gt;Low&lt;/code&gt;, and &lt;code&gt;Volume&lt;/code&gt; contain missing values (less than 0.1%); the field &lt;code&gt;Close&lt;/code&gt; has no missing values.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;There are 502 unique stocks including SPY (data for NBL, AMT, LMT, and NWL is not available) in the dataset and they belong to 11 unique sectors.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;The largest sector is Consumer Discretionary with 83 stocks, and the smallest is Telecommunication Services with only 4 stocks.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;2-2-stock-performance-example-stocks&#34;&gt;2.2 Stock Performance - Example Stocks&lt;/h3&gt;

&lt;p&gt;There are different metrics for evaluating stock performance. Each individual investor may have different standard. Overall, these a few variables people consider, for example, returns including total return, average daily return, etc., risk of the stock such as standard deviation, beta value, and risk-adjusted returns such as Sharpe ratio and alpha.&lt;/p&gt;

&lt;p&gt;As an example to examine these different metrics from historical stock data, the top 5 stocks with the highest S&amp;amp;P 500 component weights as well as &lt;strong&gt;SPY&lt;/strong&gt; are analyzed. They are &lt;strong&gt;AAPL&lt;/strong&gt; (weight 3.92%), &lt;strong&gt;MSFT&lt;/strong&gt; (weight 2.68%), &lt;strong&gt;FB&lt;/strong&gt; (weight 1.90%), &lt;strong&gt;AMZN&lt;/strong&gt; (1.83%), &lt;strong&gt;JNJ&lt;/strong&gt; (weight 1.71%). (The weights are as of August 14 2017.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Close Price&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The close price for these stocks are separated from the master dataset. The summary statistics is shown in the table below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/close_price_statistics.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Also, the correlation matrix of these stocks is shown in the figure below. Overall, the close prices of these stocks are highly correlated, with correlation coefficients close to 1. Among these stocks, Apple stock shows relatively weaker correlations with other stocks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/close_price_correlation.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Return: Simple Return&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The dataset contains 5 years of stock price data. Imagine we purchase the stock 5 years ago, we can use this dataset to calculate simple return. Simple return is the incremental amount of net income from an investment, divided by the investment in it (the price 5 years ago when we purchase it).&lt;/p&gt;

&lt;p&gt;The formular for simple return is given by:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Simple Return = (Current Price - Purchase Price) / Purchase Price&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The plot below shows stock returns over the past 5 years.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/simple_return.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This kind of plot is one of the simplest ways of comparing stock performance. It puts all different stock price to a same relative scale and makes the trend clear. As we can see, Facebook stock stands out from the plot with over 700% returns over the past five years. Amazon stock comes next to Facebook with around 300% total returns. Apple, Johnson &amp;amp; Johnson, and Microsoft stocks show returns comparable to that of SPY which is around 100% over the past five years.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Return: Annualized Return&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Annualized return is a percentage value showing how much an investment has increased in value on average per year over a period of time. It can be a preferable metric to use over simple return when we want to evaluate how successful an investment has been, or to compare the returns of two investments.&lt;/p&gt;

&lt;p&gt;Given simple return, the formula to calculate annualized return is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Annualized Return = (Simple Return + 1) ^ (1 / Years Held) - 1&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Using the simple return of 5 years, the annualized returns of these stocks can be calculated with &lt;code&gt;Years Held = 5&lt;/code&gt; in the formula above. We can then compare the annualized returns of the stocks of interest by a bar plot, as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/annualized_return.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The annualized return for SPY is slightly over 10%. Apple stock is very close to SPY. All other stocks beat SPY in terms of annualized return. Facebook and Amazon realize over 50% and over 30% annualized returns, respectively, which significantly outperform SPY.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Return: Daily Return&lt;/strong&gt;
We know that stock prices change on a daily basis. Therefore, we can also calculate daily returns to monitor the magnitude of the investment value. The daily return can be calculated with the following formula:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Daily Return = (Close Price Today - Close Price Yesterday) / Close Price Yesterday&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The summary statistics of daily return is shown in the table below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/daily_return_statistics.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can also visualize daily return with a line chart. The plot below displays the daily returns of these different stocks in a 100-day time frame.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/daily_return.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Overall, we can see that stock price fluctuates every day, like a random walk, so the daily return of a stock fluctuates around 0. All the above stocks have an average daily return slightly above 0, implying that they are overall profitable. Facebook stock price fluctuates the most, as demonstrated in the line chart as well as from its standard deviation of daily return.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Volatility: Standard Deviation&lt;/strong&gt;&lt;br /&gt;
As shown above, the daily return of Facebook stock fluctuates the most, and we say it is the most volatile among the 6 stocks. Volatility is a statistical measure of the dispersion of returns for a given stock. Volatility is commonly measured by using the standard deviation (or variance) of the returns from the stock. The daily volatility is thus the standard deviation of daily return. Commonly, the higher the volatility, the riskier the stock, i.e., the price of the stock can change dramatically over a short time period in either direction.&lt;/p&gt;

&lt;p&gt;We can compare the return and volatility of the selected stocks using the following plot. It shows daily volatility in the y-axis, and the average daily return in the x-axis. Ideally, we would hope to have a &amp;lsquo;golden&amp;rsquo; stock that lies on the lower right of the plot, but in reality, a stock with higher average daily return is usually associated with higher volatility (lying on the top right of the plot as Facebook stock). Among these stocks, Apple stock seems to have an average daily return too small for its risk.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/volatility_return.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sharpe Ratio&lt;/strong&gt;&lt;br /&gt;
The Sharpe ratio is a risk-adjusted return measurement developed by economist William Sharpe. It is calculated by subtracting the risk-free return, defined as a U.S. Treasury Bond, from the investment&amp;rsquo;s rate of return, and then dividing by the investment&amp;rsquo;s standard deviation of returns. It is useful for comparing funds with similar historical returns. A higher Sharpe ratio indicates a higher risk-adjusted return. With the daily return data, we can calculate Sharpe ratio with the following formula:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Sharpe Ratio = Mean(stock daily return - risk_free daily return) / Stdv(stock daily return - risk_free daily return)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Note that since the daily risk-free rate of return is very small (close to 0), the Sharpe ratio is roughly equal to the ratio between the average daily return and the daily volatility.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The selected stocks&amp;rsquo; Sharpe ratios (1.0% is used as the annual risk-free return for the calculation) are compared with bar plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/sharpe_ratio.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As mentioned above, Sharpe ratio is useful for comparing funds with similar historical returns. Here, let&amp;rsquo;s compare Apple stock and Johnson &amp;amp; Johnson stock. From this plot, Apple stock has a much smaller Sharpe ratio than JNJ, indicating that it has a much smaller risk-adjusted return. This is consistent with the Daily Volatility vs Average Daily Return plot, which shows that both stocks have almost the same average daily return but AAPL has much higher volatility, resulting in a smaller risk-adjusted return.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beta&lt;/strong&gt;&lt;br /&gt;
One measure of relative volatility of a particular stock to the market is its beta. Beta is a historical measure of volatility. When standard deviation measures an asset&amp;rsquo;s price movements compared to its average over time, beta measures the asset&amp;rsquo;s volatility relative to a benchmark (i.e. an index). A beta of zero implies no correlation between the assets. Any beta above zero would imply a positive correlation with volatility expressed by how much over zero the number is. Any beta below zero would imply a negative correlation with volatility expressed by how much under zero the number is.&lt;/p&gt;

&lt;p&gt;The formula for calculating beta is given by:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Beta = Covariance(stock&amp;rsquo;s daily return, index&amp;rsquo;s daily return) / Var(index&amp;rsquo;s daily return)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The calculated beta values for the selected stocks are shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/beta.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the bar plot, we can see that&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All the beta values are above zero, implying positive correlations with the volatility of SPY.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Microsoft, Apple, Amazon, and Facebook stocks have beta values greater than 1, and thus are theoretically more volatile than the benchmark SPY. For example, Amazon stock&amp;rsquo;s beta value is close to 1.2, which implies a volatility 20% greater than SPY.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Johnson &amp;amp; Johnson stock has a beta value smaller than 1 (close to 0.7), implying a volatility smaller (30% less) than the benchmark.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Alpha&lt;/strong&gt;&lt;br /&gt;
Alpha is the difference between a fund&amp;rsquo;s expected returns based on its beta and its actual returns. It is used to measure performance on a risk adjusted basis. An alpha of zero means the investment has exactly earned a return adequate for the volatility assumed. An alpha over zero means the investment has earned a return that has compensated for the volatility risk taken. An alpha of less than zero means the investment has earned a return that has not compensated for the volatility risk taken.&lt;/p&gt;

&lt;p&gt;To calculate alpha, we can use the following formula:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Alpha = Stock Return - Risk_Free Return - beta * (Index Return - Risk_Free Return)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Note that alpha and Sharp ratio have some similarities and differences, as summarized below:&lt;/em&gt;&lt;br /&gt;
- &lt;em&gt;Similarity&lt;/em&gt;: alpha and Sharpe ratio both offer a way to measure returns on a risk-adjusted basis.&lt;br /&gt;
- &lt;em&gt;Difference&lt;/em&gt;: alpha applies the measure in relation to a benchmark, whereas Sharpe ratio is based on its own standard deviation (volatility in absolute term).&lt;br /&gt;
- &lt;em&gt;Usage&lt;/em&gt;: for investors seeking an investment that closely matches the performance of a chosen benchmark, alpha is the number to review. But a fund&amp;rsquo;s R-squared must be high for alpha to be meaningful. Sharpe ratio is meaningful all the time, and it can be used to compare funds of all types, e.g., stock or bond, because standard deviation is calculated the exact same way for any type of fund but different benchmarks will be used to calculate beta, and beta-based alpha.&lt;/p&gt;

&lt;p&gt;The selected stocks alpha values are compared below (1.0% is used as the annual risk-free return):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/alpha.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that with SPY as the benchmark, all stocks except Apple stock have alpha much greater than 0. Facebook stock has the largest alpha value of 0.4, implying that it can potentially generate 40% excess return over SPY after adjusting for the inherited market risk (beta).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Moving Average - trends in a stock&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In addition to looking into the above performance metrics, we can also examine the trends in a stock by looking at the chart of the stock price. As shown above, stock data has &amp;lsquo;noises&amp;rsquo;, i.e., random price fluctuations. Moving average can be used to help smooth out short-term fluctuations and highlight longer-term trends or cycles. Given a series of numbers and a fixed subset size, the first element of the moving average is obtained by taking the average of the initial fixed subset of the number series. Then the subset is modified by &amp;ldquo;shifting forward&amp;rdquo;; that is, excluding the first number of the series and including the next value in the subset.&lt;/p&gt;

&lt;p&gt;A candlestick chart is commonly used to display the price trend as it shows all four price variables (open, high, low, and close) in one plot. Candlesticks are usually composed of the body and an upper and a lower shadow (or wick). The body illustrates the opening and closing trades. The wick illustrates the highest and lowest traded prices during the time interval represented.&lt;/p&gt;

&lt;p&gt;Below is a candlestick plot of Facebook stock in the past year with 20-day, 50-day, and 200-day moving averages plotted.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/moving_average.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The green box from the candlestick chart indicates a month of gain with the close price higher than the open price. The red box indicates a month of loss with the close price lower than the open price.&lt;/p&gt;

&lt;p&gt;From the above plot, we can see that a moving average is much smoother than the actual stock data. Moving average is thus a stronger indicator of the stock price. A stock needs to be above or below the moving average line in order for the line to change direction. Therefore, crossing a moving average signals a possible change in trend, and should draw attention. In fact, the cross-over of moving averages is sometimes used as buy/sell indicators.&lt;/p&gt;

&lt;h3 id=&#34;2-3-all-stocks-in-s-p-500&#34;&gt;2.3 All Stocks in S&amp;amp;P 500&lt;/h3&gt;

&lt;p&gt;After looking at these different aspects of the example stocks, it is time to extend the analyses to all stocks contained in the S&amp;amp;P 500 index and find out the best and worst performing stocks.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: some stocks do not have all 5-year data and are not included in the analysis.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ranks of Individual Stocks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After comparing all individual stocks contained the S&amp;amp;P 500 index, the following results are obtained:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/comparison_individual.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ranks by Sectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By aggregating the dataset by sectors and calculate the average value of the above metrics, we can rank the sectors, as shown by the plots below:&lt;/p&gt;

&lt;p&gt;Annualized Return:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/annualized_return_sector.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Volatility:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/volatility_sector.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Sharpe Ratio:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/sharpe_ratio_sector.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Beta:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/beta_sector.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Alpha:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../stock_file/alpha_sector.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;According to the plots above, the best sectors seem to be Health Care and Information Technology, in terms of high annualized return, Sharpe ratio, alpha. The Energy sector in the riskiest sector and realizes the least risk-adjusted return (both alpha, and Sharpe ratio).&lt;/p&gt;

&lt;h3 id=&#34;3-summary&#34;&gt;3. Summary&lt;/h3&gt;

&lt;p&gt;In this project, common performance metrics including returns, alpha, beta, Sharpe ratio, and moving averages are analyzed for stocks contained in the S&amp;amp;P 500 index. The price data of SPY, i.e., SPDR S&amp;amp;P 500 trust ETF (designed to track the S&amp;amp;P 500 stock market index), is used as reference for performance comparisons and also serves as benchmark for calculating alpha and beta. In addition to individual stocks, the averaged performance of different stock sectors in S&amp;amp;P 500 are also compared.&lt;/p&gt;

&lt;p&gt;Given the performance metrics computed here, it is found that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Several IT stocks performed really well in the past 5 years, including Netflix (NFLX), Nvdia (NVDA), Facebook (FB), and Electronic Arts (EA).&lt;/li&gt;
&lt;li&gt;Some Energy stocks had performance among the worst, including Chesapeake Energy (CHK), Range Resources (RRC), National Oilwell (NOV), etc.&lt;/li&gt;
&lt;li&gt;The best-performing sector is found to be Health Care and Information Technology and the worst-performing one is Energy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some next steps for stock analysis which will be covered in the future include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Design and evaluate portfolio holdings and trading strategies&lt;/li&gt;
&lt;li&gt;Build predictive machine learning models to give recommendations on trading actions&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Fraud Detection: Learning from Imbalanced Data</title>
      <link>https://yanfei-wu.github.io/blog/2017-06-20_fraud_detection/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yanfei-wu.github.io/blog/2017-06-20_fraud_detection/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The code and full report for this project is available in &lt;a href=&#34;https://github.com/yanfei-wu/kaggle/tree/master/fraud&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Combating fraud is an important application of Machine Learning. Fraud exists in many industries including banking and financial sectors, insurance, government agencies, and more. With the advances in computer technology and e-commerce, fraud attempts have seen a drastic increase in recent years, making fraud detection more important than ever. Machine learning for fraud detection works on the basis of large, historical datasets that have been aggregated across many clients. The data serve as the training set and allow businesses to build efficient machine-learning-based fraud detection systems with optimized performance. Compared to standard predictive modeling problem, fraud detection is characterized with skewed class distribution, or class imbalance, since fraud is a relatively rare event. Therefore, one challenge of fraud detection with Machine Learning is to mitigate the class imbalance problem.&lt;/p&gt;

&lt;p&gt;The goal of this project is to build supervised classification models for fraud detection using preprocessed and anonymized credit card transaction data. The main focus will be on dealing with class imbalance.&lt;/p&gt;

&lt;h2 id=&#34;2-analysis&#34;&gt;2. Analysis&lt;/h2&gt;

&lt;h3 id=&#34;2-1-data&#34;&gt;2.1 Data&lt;/h3&gt;

&lt;p&gt;The dataset (available in &lt;a href=&#34;https://www.kaggle.com/dalpozz/creditcardfraud&#34;&gt;Kaggle&lt;/a&gt; contains transactions made by credit cards in September 2013 by European cardholders. It contains only numerical input variables, including 28 features which are the result of a PCA transformation, and 1 feature which has not been transformed with PCA, namely &lt;code&gt;&#39;Amount&#39;&lt;/code&gt;. There are no missing values in the dataset. The original features and more background information about the data are not available due to confidentiality issues. The target variable for the dataset is &lt;code&gt;&#39;Class&#39;&lt;/code&gt;. It takes value 1 in case of fraud and 0 otherwise. This dataset presents transactions that occurred in two days, where there are 492 frauds out of 284,807 transactions. &lt;strong&gt;Clearly, the dataset is highly skewed, the positive class (frauds) only account for 0.172% of all transactions.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-2-methodology&#34;&gt;2.2 Methodology&lt;/h3&gt;

&lt;p&gt;Class imbalance exists in many real-world classification problems and handling imbalanced dataset has been an actively studied subject in Machine Learning. Before digging into the current problem, some common approaches for dealing with imbalanced data is reviewed below. These approaches include:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Collect more data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Having more data might lead to a more balanced dataset. A larger dataset is also helpful for minority class resampling. However, this approach is not always possible in real life.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Balance the training set.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This can be done by resampling the training set or generating synthetic samples.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Oversample the minority class.&lt;/em&gt; Oversampling randomly replicates minority instances to increase their population.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Undersample the majority class.&lt;/em&gt; Undersampling randomly downsamples the majority class.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Synthesize new minority classes.&lt;/em&gt; The best-known example of this approach is SMOTE (Synthetic Minority Oversampling TEchnique). It creates synthetic samples from the minority class instead of making duplicates. The algorithm selects two or more similar instances, i.e., nearest neighbors (using a distance measure), and perturbs an instance one attribute at a time by a random amount within the difference to the neighboring instances to construct new minority instances.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Resampling is the easiest to implement. However, both under/over-sampling have their drawbacks. Undersampling discards potentially important data and makes the independent variables look more variable than they actually are; whereas data duplication with oversampling makes the variable appear to have low variance and could lead to overfitting. SMOTE, compared to traditional oversampling, creates similar examples instead of exact copies of the minority class, which results in more general models and avoids overfitting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Change the performance metrics.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The performance of classification algorithms is typically evaluated by a confusion matrix which compares the predicted class and the actual class. In the confusion matrix, the number of negative examples correctly classified is True Negatives (TN), the number of negative examples incorrectly classified as positive is False Positives (FP), the number of positive examples incorrectly classified as negative is False Negatives (FN) and the number of positive examples correctly classified is True Positives (TP). In the context of balanced datasets and equal error costs, the common performance metric is predictive accuracy, which is defined as the percentage of correctly identified examples, i.e., (TP+TN)/(TP+FP+TN+FN). In the case of imbalanced datasets, it is more appropriate to use other metrics.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Precision-recall curve.&lt;/em&gt; Precision is defined as TP/(TP+FP) and is a measure of the classifier&amp;rsquo;s exactness. Recall, on the other hand, is defined as TP/(TP+FN). It measures the classifier&amp;rsquo;s sensitivity. There is a tradeoff between precision and recall. For a given model, the classifier performance can be visualized using a precision-recall curve, which captures the tradeoff between precision and recall as the decision threshold is varied. On its own, a model is overall better if the curve is closer to the top right of the chart where precision and recall are both 1.0. One way to capture the overall quality of the model is by computing the area under the curve (AUC). The AUC will be 1.0 for an ideal model.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that Receiver Operator Characteristic (ROC) curve is also commonly used to measure the performance of binary classifiers. However, when dealing with highly skewed datasets, it provides a less informative picture of the algorithm&amp;rsquo;s performance than the precision-recall curve.[1]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Algorithm level approach.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Adjust the class weight.&lt;/em&gt; This approach is based on reducing the misclassification costs for minority class. Many Machine Learning toolkits have ways to adjust the importance of the classes. For examples, many classifiers in &lt;code&gt;Scikit-learn&lt;/code&gt; have an optional &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; parameter that can be set to &amp;lsquo;balanced&amp;rsquo;.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Adjust the decision threshold.&lt;/em&gt; Instead of getting classification labels from the classifier, a good approach to handling class imbalance is to estimate probability and then adjust the decision threshold to separate classes. Probabilistic classification models in &lt;code&gt;Scikit-learn&lt;/code&gt; such as logistic regression offer &lt;code&gt;&#39;predict_proba&#39;&lt;/code&gt; method to estimate probability.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-results-and-discussions&#34;&gt;3. Results and Discussions&lt;/h2&gt;

&lt;h3 id=&#34;3-1-model-selection-and-effect-of-adjusting-class-weight&#34;&gt;3.1 Model Selection and Effect of Adjusting Class Weight&lt;/h3&gt;

&lt;p&gt;As mentioned above, for classification problem with skewed data, it is better to estimate probability than labels. So several naturally probabilistic classification algorithms are selected here, including Naive Bayes, Logistic Regression, Decision Tree Classifier, and Random Forest. Also, except for Naive Bayes, all other models have the &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; parameter that can be set to &amp;lsquo;balanced&amp;rsquo;. So before any resampling of the data, the performance of these models (with or without setting &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; to be &amp;lsquo;balanced&amp;rsquo;) is compared.&lt;/p&gt;

&lt;p&gt;The dataset is first randomly split into 80% training set and 20% test set. The features are normalized before training the models. The precision-recall curves of different models (Figure 2) and their AUC scores (Figure 3) are shown below. Since a model is considered better if the curve is closer to the top right of the chart, in this particular case, Random Forest model with &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; set to be &amp;lsquo;balanced&amp;rsquo; seems to be the best. The comparison of areas under the precision-recall curves also shows that Random Forest model (balanced) gives the largest area followed by Random Forest model (not balanced), and logistic model (not balanced). Interestingly, although adjusting class weight shows a positive effect on Random Forest model, it does not result in significant change of the model performance for Logistic Regression, and it even exerts a negative effect (worse precision-recall performance) for Decision Tree Classifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fraud_file/pr.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fraud_file/auc.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-2-effect-of-smote&#34;&gt;3.2 Effect of SMOTE&lt;/h3&gt;

&lt;p&gt;An alternative to adjusting class weight at the algorithm level is to balance the training set, for example, with SMOTE. As introduced above, SMOTE is an oversampling technique, but instead of creating exact copies of the existing minority data, SMOTE generates new, synthetic minority points by interpolating between current minority examples.&lt;/p&gt;

&lt;p&gt;A Python package named &lt;a href=&#34;http://contrib.scikit-learn.org/imbalanced-learn/index.html#&#34;&gt;&lt;code&gt;&#39;imbalanced-learn&#39;&lt;/code&gt;&lt;/a&gt; is used to implement SMOTE. The package offers a number of re-sampling techniques commonly used in datasets showing strong between-class imbalance. SMOTE operation generates additional positive examples so that the class ratio of the training set becomes 1:1. The above four classification algorithms, Naive Bayes, Logistic Regression, Decision Tree, and Random Forest (all out-of-box and without setting &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; to &amp;lsquo;balanced&amp;rsquo;) are trained using the new balanced training set. The models are then compared with their precision-recall curves (Figure 4) and AUC scores (Figure 5). The best model is again out-of-box Random Forest model. The AUC score of the Random Forest model after SMOTE (0.852) is better than the one without SMOTE (0.849), but is not as good as Random Forest model with adjusting class weight (0.861). In other words, both approaches improve the performance of the model but in this particular case, adjusting the class weight is slightly better than SMOTE. Note that SMOTE is limited in the fact that it can only generate examples WITHIN the body of available examples, i.e., it can only fill in the convex hull of existing minority examples but not outside.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fraud_file/pr_smote.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fraud_file/auc_smote.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-conclusion&#34;&gt;4. Conclusion&lt;/h2&gt;

&lt;p&gt;This project explores approaches for handling class imbalance by building supervised fraud detection models using credit card transaction data. The original dataset is highly skewed with the positive class only accounting for 0.172%. Therefore, instead of using common metrics like accuracy, the precision-recall curve and the area under the curve (AUC) are used as the performance measure for the models. Algorithm level approach (i.e., adjusting class weight) and data rebalancing technique (SMOTE) are used to optimize the model performance. It is found that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Both adjusting class weight and SMOTE result in better model performance as measured by the precision-recall curve.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Random Forest model turns out to be the best performing model compared to Naive Bayes, Logistic Regression, and Decision Tree Classifier.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Adjusting class weight of Random Forest model gives slightly better result than training the model on SMOTE-balanced dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;[1] J. Davis et al. The relationship between Precision-Recall and ROC curves. Proceedings of the 23rd international conference on Machine learning, 2006.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Employee Retention</title>
      <link>https://yanfei-wu.github.io/blog/2017-04-28_employee_retention/</link>
      <pubDate>Fri, 28 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yanfei-wu.github.io/blog/2017-04-28_employee_retention/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The code and full report for this project is available in &lt;a href=&#34;https://github.com/yanfei-wu/ml_udacity/tree/master/capstone&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h3&gt;

&lt;p&gt;Recruiting excellent employees is one thing, but keeping them is another. While losing employees who are poor performers can have positive effects, high employee turnover rate is generally regarded as bad for the business. It increases expenses since the process of identifying, hiring and training employees is very expensive. Studies have shown that cost related to directly replacing an employee can be as high as 50–60% of the employee’s annual salary, and the total cost of turnover can reach as high as 90–200% of the employee’s annual salary.[1] Even worse, frequent employee turnover can destroy the company morale, resulting in decreased performance in the workplace. Therefore, retaining its valuable and talented employees is vital to a company&amp;rsquo;s success.&lt;/p&gt;

&lt;p&gt;A novel approach to implementing an effective retention program and preventing key workers from leaving prematurely is to use machine learning techniques. For example, a supervised classification model can be trained on a dataset containing features related to the employees and whether they left the company (a dataset that is available to many companies). Building such a model provides insights to key factors that result in employee turnover. It also allows the management and the human resource team to predict which employee is going to leave so that they could intervene immediately.&lt;/p&gt;

&lt;p&gt;The goal of this project is to build such a supervised binary classification model. A simulated dataset (available in &lt;a href=&#34;https://www.kaggle.com/ludobenistant/hr-analytics&#34;&gt;Kaggle&lt;/a&gt;) containing a series of employee-related features and a binary class label of whether the employee left or not is used. The expected outcome of this project is to help the management and the human resource team:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;predict which current employee is going to leave (class label) so that they could intervene immediately;&lt;/li&gt;
&lt;li&gt;identify which are the most important factors (features) that lead to employee turnover so that changes can be implemented to ensure employees remain in place while maintaining high work performance and productivity.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;2-datasets-and-exploratory-visualization&#34;&gt;2. Datasets and Exploratory Visualization&lt;/h3&gt;

&lt;p&gt;The dataset contains 14,999 rows and 10 columns. There are both numerical and categorical features in this dataset, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;satisfaction_level&#39;&lt;/code&gt;: employee satisfaction level (numerical, float numbers between 0 and 1)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;last_evaluation&#39;&lt;/code&gt;: the score from the employee&amp;rsquo;s last evaluation (numerical, float numbers between 0 and 1)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;number_project&#39;&lt;/code&gt;: the number of projects the employee worked on (numerical, integers)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;average_monthly_hours&#39;&lt;/code&gt;: the average monthly hours the employee spent on work (numerical, integers)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;time_spend_company&#39;&lt;/code&gt;: number of year the employee spent at the company (numerical, integers)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;work_accident&#39;&lt;/code&gt;: whether the employee had a work accident (categorical, &amp;lsquo;0&amp;rsquo; - no, &amp;lsquo;1&amp;rsquo; - yes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;promotion_last_5year&#39;&lt;/code&gt;: whether the employee had a promotion in the last 5 years (categorical, &amp;lsquo;0&amp;rsquo; - no, &amp;lsquo;1&amp;rsquo; - yes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;sales&#39;&lt;/code&gt;: the Department the employee works (categorical, &amp;lsquo;sales&amp;rsquo;, &amp;lsquo;accounting&amp;rsquo;, &amp;lsquo;hr&amp;rsquo;, &amp;lsquo;technical&amp;rsquo;, &amp;lsquo;support&amp;rsquo;, &amp;lsquo;management&amp;rsquo;, &amp;lsquo;IT&amp;rsquo;, &amp;lsquo;product_mng&amp;rsquo;, &amp;lsquo;marketing&amp;rsquo;, &amp;lsquo;RandD&amp;rsquo;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;salary&#39;&lt;/code&gt;: the salary of the employee (categorical, &amp;lsquo;low&amp;rsquo;, &amp;lsquo;medium&amp;rsquo;, &amp;lsquo;high&amp;rsquo;)&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The target variable in the dataset is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;left&#39;&lt;/code&gt;: whether the employee has left or not (categorical, &amp;lsquo;0&amp;rsquo; - no, &amp;lsquo;1&amp;rsquo; - yes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Note that the number of employees who left in the dataset is 3571 and is 23.81% of the total employees (14,999).&lt;/em&gt; As discussed above, with the unbalanced class distributions, care must be taken when choosing performance measure for the models. F2 score will be used in order to weight recall higher than precision.&lt;/p&gt;

&lt;p&gt;Also note is that there are no &lt;code&gt;NaN&lt;/code&gt; values and all the numerical values are within reasonable range. No outliers are detected.&lt;/p&gt;

&lt;p&gt;Both the numerical and the categorical features are visualized below.&lt;br /&gt;
&lt;img src=&#34;../employee_file/numerical.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Some observations from the numerical features are&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;time_spend_company&lt;/code&gt; and &lt;code&gt;number_project&lt;/code&gt; take integer values. This dataset only considers employees at the company for 2 or more years with 2 or more projects.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;average_monthly_hours&#39;&lt;/code&gt;, &lt;code&gt;&#39;last_evaluation&#39;&lt;/code&gt;, and &lt;code&gt;&#39;satisfaction_level&#39;&lt;/code&gt; take continuous values. All of them show bimodal distributions.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;../employee_file/categorical.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Some Observations from the categorical features are&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;work_accident&#39;&lt;/code&gt; and &lt;code&gt;&#39;promotion_last_5years&#39;&lt;/code&gt; are two-level categorical variables. Both show very unbalanced distribution (with the majority being category 0: no work accident or no promotion in last 5 years)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;salary&#39;&lt;/code&gt; has 3 levels: low, medium and high. The salary structure seems to reflect the reality, with the majority of employees having low and medium salaries.&lt;/li&gt;
&lt;li&gt;Employees from 10 different departments are included in the dataset, with the majority of employees from sales, technical and support. This also somewhat reflects the reality in many companies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A correlation matrix of the dataset (excluding &lt;code&gt;&#39;department&#39;&lt;/code&gt;) is shown below.&lt;br /&gt;
&lt;img src=&#34;../employee_file/correlationmatrix.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The correlation matrix shows that:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There is a strong negative correlation between &lt;code&gt;&#39;satisfaction_level&#39;&lt;/code&gt; and &lt;code&gt;&#39;left&#39;&lt;/code&gt; with a correlation coefficient of -0.39.&lt;/li&gt;
&lt;li&gt;Variables &lt;code&gt;&#39;number_project&#39;&lt;/code&gt;, &lt;code&gt;&#39;average_monthly_hours&#39;&lt;/code&gt;, and &lt;code&gt;&#39;last_evaluation&#39;&lt;/code&gt; show strong positive correlations, which makes sense intuitively.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;4-models&#34;&gt;4. Models&lt;/h3&gt;

&lt;h4 id=&#34;4-1-algorithms-and-techniques&#34;&gt;4.1 Algorithms and Techniques&lt;/h4&gt;

&lt;p&gt;It is a binary classification problem and there are a number of algorithms available in the &lt;code&gt;scikit-learn&lt;/code&gt; library. To find the best model for the problem, 6 different algirithms are used to train the dataset, including Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbors, Decision Trees, Support Vector Machines, and Random Forest. All the algorithms are evaluated out-of-box, i.e., default hyperparameters will be used. A naive benchmark model is also built to set a baseline for model performance. The benchmark model is to predict that all the employees are leaving. In other words, the management and HR team treat every single employee as if he/she is leaving. Note that implementing such a naive classifier will probably lower the employee turnover rate but most of the time, it is not a realistic retention program for most companies. So, it is only for comparison purpose here.&lt;/p&gt;

&lt;p&gt;Typically, the performance of a classification model can be measured by accuracy, i.e., the percentage of correct predictions. But in this case, it is not the best metric due to the class imbalance (~25% class &amp;lsquo;left&amp;rsquo; vs ~75% class &amp;lsquo;stay&amp;rsquo;). Instead, we can use a combination of precision and recall as an unambiguous way to show the prediction results. In this specific problem, we would like to avoid predicting valuable employees who are actually leaving as staying and thus taking no actions. Therefore, the so-called F2 score is used. It is a weighted average of precision and recall and it places more emphasis on recall (minimizing False Negatives).&lt;/p&gt;

&lt;p&gt;Note that before the dataset can be used to train the classificatin models, some preprocessing steps are done, including one-hot encoding of the categorical features and normalization of the numerical features. The latter ensures that each feature is treated equally when applying supervised learners.&lt;/p&gt;

&lt;h4 id=&#34;4-2-model-selection-and-feature-importance&#34;&gt;4.2 Model Selection and Feature Importance&lt;/h4&gt;

&lt;p&gt;In order to evaluate each algorithm, a 10-fold cross validation procedure is carried out. The cross validation F2 scores for the 6 models are shown in the boxplot below. The F2 score of the benchmark model is also plotted as a baseline.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../employee_file/modelcomparison.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Based on the comparison, Decision Trees classifier and Random Forest both perform very well. They give much better F2 score than the benchmark classifier. Generally, Random Forest algorithm is less likely to overfit the data compared to Decision Trees. So in this case, it is chosen as the best model.&lt;/p&gt;

&lt;p&gt;The Random Forest model is further tuned with &lt;code&gt;GridSearchCV&lt;/code&gt;. The best 10-fold cross validation F2 score is 0.9754, which shows slight improvevment than the score from the out-of-box Ramdom Forest model (0.9660).&lt;/p&gt;

&lt;p&gt;With a classification model like the Random Forest classifier built above, management and human resource team are now able to predict if a current employee is leaving so that they can take immediate internvention. Another key information for an effective employee retention program is the knowledge of the most important factors on employee turnover. Luckily, with Random Forest classifier, we can easily obtain the feature importance from a trained model. A barplot of the top 5 features from the optimized Random Forest model trained on the entire training set is shown below.&lt;/p&gt;

&lt;p&gt;Thses 5 features consists about &lt;strong&gt;95%&lt;/strong&gt; of the total feature importance. &lt;code&gt;&#39;satisfaction_level&#39;&lt;/code&gt; is determined to be the most important feature, which is consistant with its high correlation coefficient with the target variable &lt;code&gt;&#39;left&#39;&lt;/code&gt; as shown in the correlation matrix. The other features highly relevant to employee turnover are &lt;code&gt;&#39;average_monthly_hours&#39;&lt;/code&gt;, &lt;code&gt;&#39;number_project&#39;&lt;/code&gt;, &lt;code&gt;&#39;time_spend_company&#39;&lt;/code&gt;, and &lt;code&gt;&#39;last_evaluation&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../employee_file/featureimportance.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h3&gt;

&lt;p&gt;To summarize, this project outlines a machine learning approach to help implement an effective employee retention program. A synthesized dataset containing both numerical and categorical features of the employees, as well as a binary label of whether the employee left or not is used to build a binary classification model. The categorical features are transformed to numerical values by one-hot encoding. The numerical features are rescaled to the same range (0 to 1) so that the learners can treat each feature equally. Different classification algorithms are compared. The performance of the models are measured by the F2 score in order to put more weights on recall. K fold cross validation procedure is implemented to evaluate the models. Random Forest classifier is chosen as it gives the best cross validation F2 score. The model is further tuned using &lt;code&gt;GridSearchCV&lt;/code&gt;. The final model shows excellent F2 score (much better than the naive benchmark classifier) and appears to be generalizable to unseen data. With this classification model, we can predict whether a current employee will leave given the features of the employee. Feature importance analysis reveals the top 5 features, which consist about 95% of the total importance. These top features can help management and human resource team understand why their emplpyees leave.&lt;/p&gt;

&lt;p&gt;However, the prediction results seem too good to be true in reality. But data leakage should not be a problem here since feature normalization is performed on each fold of cross validation instead of on the entire dataset. So, one possible explanation for this exceptionally good result could be that the synthesized dataset is just too ideal. The real-world data is normally a lot messier. Also, it is reasonable to believe that employee turnover characteristic would be different for each individual company, or at least for each type of industry. Therefore, this specific model obtained here is likely not suitable for specific companies out there. Nevertheless, the general machine learning approach described here should be a good practice in reality.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;[1] Cascio, W.F. 2006. Managing Human Resources: Productivity, Quality of Work Life, Profits (7th ed.). Burr Ridge, IL: Irwin/McGraw-Hill. Mitchell, T.R., Holtom, B.C., &amp;amp; Lee, T.W. 2001. How to keep your best employees&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lines from South Park</title>
      <link>https://yanfei-wu.github.io/blog/2017-03-20_southpark/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yanfei-wu.github.io/blog/2017-03-20_southpark/</guid>
      <description>

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;South Park is an adult animated sitcom about four potty-mouthed grade boys Eric Cartman, Stan Marsh, Kyle Broflovshi, and Kenny McCormick and their adventures in a fictional Colorado town called South Park. The show debuted in 1997 and has been through 20 seasons now. It is such a famous show and it has won a lot of awards but for some reason I never really watched this show (except for a couple YouTube clips).&lt;/p&gt;

&lt;p&gt;Recently, I found a dataset from &lt;a href=&#34;https://www.kaggle.com/tovarischsukhov/southparklines&#34;&gt;Kaggle&lt;/a&gt; with the lines of the first 18 seasons. These lines have been annotated with season, episode and speaker. I was curious about what I can find out about the show from this dataset. So here it goes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; All the codes for this post can be found in &lt;a href=&#34;https://github.com/yanfei-wu/tv_lines&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;get-to-know-the-show&#34;&gt;Get to Know the Show&lt;/h3&gt;

&lt;p&gt;First, I looked at some basic information about the show based on this dataset, such as the number of episodes, the number of characters, the number of lines, etc. The information is summarized in the table below:&lt;/p&gt;

&lt;div&gt;
&lt;table border=&#34;1&#34; align=&#34;center&#34;&gt;
  &lt;thead&gt;
    &lt;tr align=&#34;center&#34;&gt;
      &lt;th&gt; Seasons &lt;/th&gt;
      &lt;th&gt;  Episodes  &lt;/th&gt;
      &lt;th&gt;  Characters  &lt;/th&gt;
      &lt;th&gt;  Lines  &lt;/th&gt;
      &lt;th&gt;  Words  &lt;/th&gt;
      &lt;th&gt;  Words (Unique)  &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr align=&#34;center&#34;&gt;
      &lt;td width=&#34;15%&#34;&gt; 18 &lt;/td&gt;
      &lt;td width=&#34;15%&#34;&gt; 257 &lt;/td&gt;
      &lt;td width=&#34;15%&#34;&gt; 3,949 &lt;/td&gt;
      &lt;td width=&#34;15%&#34;&gt; 70,789 &lt;/td&gt;
      &lt;td width=&#34;15%&#34;&gt; 811,296 &lt;/td&gt;
      &lt;td width=&#34;20%&#34;&gt; 30,347 &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;  

&lt;p&gt;We can see that there are over 70,000 lines in the dataset. So how long are these lines? The number of words in a line is used to define the line length. The distribution of the line length shows that the majority of the lines in the show are relatively short (with a median of 8 words and a 75 percentile of 14 words).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/linelength_dist.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Another question is how did the show evolve over the seasons in terms of the number of episodes, characters, and lines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/over_season.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Based on the plot above, the show started off with 13 episodes and around 250 characters in Season 1. On average, it had about 320 lines per episode but the lines seem to be short. The first season was clearly a success as we can see a big jump in the numbers of episodes and characters as well as the average numbers of lines and words in Season 2. Overall, these numbers peaked from Season 2 through Season 4 followed by some slow decay. During Season 10 to Season 15, more characters were included in each season although each season remained to have 14 episodes. The average number of lines, interestingly, did not show a corresponding increase. But the lines were definitely getting longer on average from Season 13.&lt;/p&gt;

&lt;h3 id=&#34;get-to-know-the-characters&#34;&gt;Get to Know the Characters&lt;/h3&gt;

&lt;h4 id=&#34;who-spoke-the-most&#34;&gt;Who Spoke the Most?&lt;/h4&gt;

&lt;p&gt;We know that the story is about the four boys - Cartman, Stan, Kyle, and Kenny. Let&amp;rsquo;s see if that is what the lines are telling us. But first, we can see who actually appeared in all 18 seasons. It turned out that they are: &lt;em&gt;Stan, Kyle, Cartman, Gerald, Mr. Mackey, Randy, Kenny, Clyde, Sheila, Sharon, Wendy, Principal Victoria, and Liane (and also voices annotated as &amp;lsquo;Woman&amp;rsquo;, &amp;lsquo;Man&amp;rsquo;, and &amp;lsquo;Announcer&amp;rsquo;)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Then, let&amp;rsquo;s see who spoke the most. We can check both the total number of lines and the number of words for each character.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/topspeaker.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly, Cartman is No. 1, followed by Stan and Kyle. Kenny, however, falls out of the top 5. Instead, Butters and Randy are in the top 5 list although their line shares are significantly less than the other three. Also something to note is Butters actually did not appear in all 18 seasons. He was introduced into the show in Season 2.&lt;/p&gt;

&lt;p&gt;So from now on, we can just focus on the top 5 speakers. The changes of their word shares over the seasons are shown below. Cartman remained to be No. 1 throughout the seasons, taking up to around 15% of the word shares. Stan and Kyle started off similarly like Cartman but their word shares decreased over time. Butters and Randy did not speak much in the first 4 seasons but they rise to be among the top speakers and ended up having similar word shares as Stan and Kyle, around 5%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/wordshare.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/linelength.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The average lengths of the lines (defined as the total number of words divided by total number of lines) for Cartman, Stan, Kyle and Randy have not changed dramatically over the seasons. Also, their line lengths are relatively close, 10-15 words per line. But for Butters, his average line length shows a huge jump from Season 2 (when he was first introduced to the show) to season 3. I was very curious about exactly how this character was introduced. What did he say in Season 2? It turned out that he only said two lines:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Me, too!&amp;rdquo;&lt;br /&gt;
 &amp;ldquo;Pass this up.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;what-did-top-speakers-say&#34;&gt;What did Top Speakers Say?&lt;/h4&gt;

&lt;p&gt;So what exactly did the characters say? A word cloud from &lt;em&gt;all the lines&lt;/em&gt; is shown as the cover image of this post. But I am more interested in the &lt;em&gt;lines of the top speakers&lt;/em&gt;. What are their most frequent words and the most characteristic words?&lt;/p&gt;

&lt;p&gt;Finding out the most frequent words is simply a word count problem. A note here is that stopwords were removed. Below I show the top 20 most frequent words for our top 5 speakers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/frequent_word.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Compared to knowing the most frequent words, finding out the most characteristic words for each speaker is probably more interesting. Here, &lt;strong&gt;term frequency-inverse document frequency&lt;/strong&gt; or &lt;strong&gt;tf-idf&lt;/strong&gt; is used. It is often used in information retrieval and text mining as a measure to evaluate how important a word is to a document in a corpus. The tf-idf weight is composed by 2 components. one is the term frequency (tf) and it is simply the number of times term t appears in a document divided by the total number of terms in the corpus. The other component is the inverse document frequency (idf) and it is the natural log of the ratio between the total number of documents and the number of documents with term t in it. The final tf-idf weight is calculated as the product between tf and idf. So a term is more important to a document if it appears in this document for many times (large tf) but it does not appear in many of the documents in the corpus (large idf).&lt;/p&gt;

&lt;p&gt;For our top 5 speakers, their most important words and phrases are shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/important_word.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/important_phrase.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There are some swear words, as you can see. And Kenny apparently had been killed many times!&lt;/p&gt;

&lt;h4 id=&#34;what-did-kenny-say&#34;&gt;What did Kenny Say?&lt;/h4&gt;

&lt;p&gt;As one of the four main characters, Kenny had significantly small word shares in the show. So who is Kenny? What did he do in the story? Do these lines tell anything about what he is like? We can simply construct a word cloud using Kenny&amp;rsquo;s lines in the dataset (with stopwords removed). Apparently, the F word is among the top of Kenny&amp;rsquo;s vocabulary, along with other swear words!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/kenny_wordcloud.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The word cloud is based on word frequency. Another way to look at this is the importance of the words. The most important 20 words in Kenny&amp;rsquo;s lines with the highest tf-idf weights are shown below. We know that the show is known for its use of profanity. It seems that Kenny, one of the main characters in the show who spoke the least mainly spoke profanity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/topwords_kenny.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;In this post, I walked you through my exploration of the South Park lines dataset and showed some interesting facts I learned about the show and the characters from the lines. I guess I will go watch a couple of episodes now :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>&#34;Toxic Beauty&#34;: Chemicals in Cosmetics</title>
      <link>https://yanfei-wu.github.io/blog/2017-03-15_chemicals/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yanfei-wu.github.io/blog/2017-03-15_chemicals/</guid>
      <description>

&lt;p&gt;How many different beauty products do you apply onto your face and body everyday? How many chemical ingredients do they contain? And how many of those chemicals have potential health risks?&lt;/p&gt;

&lt;p&gt;I admit that I have been a little careless when it comes to the safety of my cosmetic products. Then one day I came across this &lt;a href=&#34;https://www.healthdata.gov/dataset/chemicals-cosmetics&#34;&gt;dataset&lt;/a&gt; from California Safe Cosmetics Program in California Department of Public Health. It appears that companies (manufacturer, packer, and/or distributor named on the product label) must submit a list of all products that contain any ingredients &lt;strong&gt;known or suspected to cause cancer, birth defects, or other developmental or reproductive harm&lt;/strong&gt; if the company has annual aggregate sales of cosmetic products of one million dollars or more, and has sold cosmetic products in California on or after January 1, 2007. So I started to dig into this dataset because I was very curious about what exactly were reported. Let me walk you through what I did. All codes are available in &lt;a href=&#34;https://github.com/yanfei-wu/chemical_cosmetic&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;data-cleaning&#34;&gt;Data Cleaning&lt;/h3&gt;

&lt;h4 id=&#34;missing-data&#34;&gt;Missing Data&lt;/h4&gt;

&lt;p&gt;The dataset has 80,004 rows and 22 columns. We can visualize columns containing missing data using a heatmap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../chemicals_file/missingdata.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that the majority of the rows in the &lt;code&gt;&#39;DiscontinuedDate&#39;&lt;/code&gt; and &lt;code&gt;&#39;ChemicalDateRemoved&#39;&lt;/code&gt; columns are missing. &lt;code&gt;&#39;CSFId&#39;&lt;/code&gt; and &lt;code&gt;&#39;CSF&#39;&lt;/code&gt; also contains a lot of missing data, followed by &lt;code&gt;&#39;CASNumber&#39;&lt;/code&gt; and &lt;code&gt;&#39;BrandName&#39;&lt;/code&gt;. We know that typical ways of dealing with missing data include a. deleting rows/columns with missing values and b. filling the missing values with some value. The first approach is the easist but caution needs to be taken because we can potentially lose some important information. In our case, the &lt;code&gt;&#39;DiscontinuedDate&#39;&lt;/code&gt; and &lt;code&gt;&#39;ChemicalDateRemoved&#39;&lt;/code&gt; columns give information about product discontinuation or reformulation. So here they are probably just not applicable (products still on sale and no reformulation has been done). We can leave them as they are. For the missing &lt;code&gt;&#39;CSFId&#39;&lt;/code&gt;, &lt;code&gt;&#39;CSF&#39;&lt;/code&gt; and &lt;code&gt;&#39;CasId&#39;&lt;/code&gt;, I do not have enough knowledge to fill in these values but I do not want to lose records by deleting corresponding rows. So again, let&amp;rsquo;s leave them as they are. &lt;code&gt;&#39;BrandName&#39;&lt;/code&gt; is actually something interesting. We can try to fill in the missing values using information given in the dataset, e.g., &lt;code&gt;&#39;CompanyName&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../chemicals_file/missingdata_company.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These products with a missing &lt;code&gt;&#39;BrandName&#39;&lt;/code&gt; actually come from 6 companies and two of them are cosmetic companies. Based on a quick googling, they each carries one brand only (brand name is the same as the company name). So we can simply fill in the brand names for products from these two companies. In the remaining companies, two of them (Fisk Industries Inc. and Atlas Development Limited) only have a small number of records. So it is reasonable to look at them individually and fill in the missing brand names based on the brands they carry. For the remaining records, they can actually be filled with &amp;ldquo;Unknown&amp;rdquo; for simplicity.&lt;/p&gt;

&lt;h4 id=&#34;duplicates-and-unique-values&#34;&gt;Duplicates and Unique Values&lt;/h4&gt;

&lt;p&gt;The other problem a public dataset might have is duplicates. Note that before checking duplicates, the inconsistent format of string values (upper and lower cases are mixed) is corrected. 251 duplicated rows are found. Further inspection of these rows does not show any connection or anything in common. So they are simply dropped.&lt;/p&gt;

&lt;p&gt;The unique number of values for several columns are listed below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Unique number of &lt;code&gt;&#39;CDPHId&#39;&lt;/code&gt;: 26,036&lt;/li&gt;
&lt;li&gt;Unique number of &lt;code&gt;&#39;CompanyName&#39;&lt;/code&gt;: 508&lt;/li&gt;
&lt;li&gt;Unique number of &lt;code&gt;&#39;BrandName&#39;&lt;/code&gt;: 1,820&lt;/li&gt;
&lt;li&gt;Unique number of &lt;code&gt;&#39;ProductName&#39;&lt;/code&gt;: 23,992&lt;/li&gt;
&lt;li&gt;Unique number of &lt;code&gt;&#39;PrimaryCategory&#39;&lt;/code&gt;: 13&lt;/li&gt;
&lt;li&gt;Unique number of &lt;code&gt;&#39;SubCategory&#39;&lt;/code&gt;&amp;lsquo;: 89&lt;/li&gt;
&lt;li&gt;Unique number of &lt;code&gt;&#39;ChemicalName&#39;&lt;/code&gt;: 110&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further inspection of the dataset shows that &lt;code&gt;&#39;CDPHId&#39;&lt;/code&gt; represents unique product. So the difference between number of &lt;code&gt;&#39;ProductName&#39;&lt;/code&gt; and &lt;code&gt;&#39;CDPHId&#39;&lt;/code&gt; indicates that there are products with the same name. We can actually generate a seperate dataframe storing the basic information (&lt;code&gt;&#39;CDPHId&#39;&lt;/code&gt;, &lt;code&gt;&#39;CompanyName&#39;&lt;/code&gt;, &lt;code&gt;&#39;BrandName&#39;&lt;/code&gt;, &lt;code&gt;&#39;ProductName&#39;&lt;/code&gt;) of unique products.&lt;/p&gt;

&lt;h3 id=&#34;data-visualization&#34;&gt;Data Visualization&lt;/h3&gt;

&lt;p&gt;There are several questions I am interested in. For example:&lt;br /&gt;
1. What are the most common reportable chemicals?&lt;br /&gt;
2. Which products have the largest number of reportable chemicals?&lt;br /&gt;
3. Which categories of products are reported most frequently?&lt;br /&gt;
4. Which companies/brands have the largest number of products reported?&lt;/p&gt;

&lt;h4 id=&#34;most-common-reportable-chemicals&#34;&gt;Most common reportable chemicals&lt;/h4&gt;

&lt;p&gt;The dataset is grouped by &lt;code&gt;&#39;ChemicalName&#39;&lt;/code&gt; and the number of unique &lt;code&gt;&#39;CDPHId&#39;&lt;/code&gt; is counted.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../chemicals_file/most_common_chemical.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Looks like titanium dioxide is contained in the majority of the products reported. The exact harm of titanium dioxide is not well understood. It is generally considered to be a relatively inert, safe material, but when it is in the form of nanoparticles, some believe that it can penetrate the skin and cause potential health risk. Currently, only airborne, unbound titanium dioxide particle of respirable size is officially listed in &lt;a href=&#34;https://oehha.ca.gov/proposition-65/about-proposition-65&#34;&gt;California Proposition 65&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All other chemicals are contained in much smaller number of products. The health risks for some of them are also controversial. FDA actually only takes legal action against cosmetic companies if there is enough evidence of a safety problem for consumers.&lt;/p&gt;

&lt;h4 id=&#34;products-with-the-largest-number-of-reportable-chemicals&#34;&gt;Products with the largest number of reportable chemicals&lt;/h4&gt;

&lt;p&gt;First, let&amp;rsquo;s look at some statistics about the number of reportable chemicals in the products:&lt;br /&gt;
- Minimum number of reportable chemicals in any product: 1&lt;br /&gt;
- Maximum number of reportable chemicals in any product: 10&lt;br /&gt;
- Median number of reportable chemicals in all product: 1.0&lt;br /&gt;
- Mean number of reportable chemicals in all product: 1.0970579198033492&lt;br /&gt;
- Standard deviation of the number of reportable chemicals in all product: 0.3771734727730194&lt;/p&gt;

&lt;p&gt;Looks like the majority of products in the dataset only contains one reportable chemical (highly likely to be titanium dioxide), but there are also products contain more than one reportable chemicals. What are they?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../chemicals_file/top_product.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A lot of them are hair and skin care products!&lt;/p&gt;

&lt;h4 id=&#34;categories-of-products-reported-most-frequently&#34;&gt;Categories of products reported most frequently&lt;/h4&gt;

&lt;p&gt;Similarly, we can group the dataset by &lt;code&gt;&#39;PrimaryCategory&#39;&lt;/code&gt; and count the number of unique products.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../chemicals_file/category.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Makeup products is No. 1. This is not surprising because most of them contain chemicals like titanium dioxide, mica, talc, etc.&lt;/p&gt;

&lt;h4 id=&#34;companies-brands-with-the-largest-number-of-products-reported&#34;&gt;Companies/Brands with the largest number of products reported&lt;/h4&gt;

&lt;p&gt;Another thing to look at is which companies/brands have the largest number of products reported. Again, simply group the data by &lt;code&gt;&#39;CompanyName&#39;&lt;/code&gt; or &lt;code&gt;&#39;BrandName&#39;&lt;/code&gt; and count the number of unique products.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../chemicals_file/company.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;../chemicals_file/brand.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The potential risks of a lot of the chemicals are actually controversial and FDA does not take legal action against cosmetic companies unless there is enough evidence. So it is really up to us to decide whether we want to avoid some ingredients or not.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Compared to products with only one chemical reported like titanium dioxide (commonly used in sunscreens and makeups; probably hard to avoid), we should probably be more cautious about products containing several different kinds of potentially dangerous chemicals.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;According to the dataset, makeup is the No.1 category with the most products reported to have potentially dangerous chemicals and baby products seem to be the &amp;ldquo;safest&amp;rdquo;. The former is not surprising to most of us and the latter is reassuring!&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: the dataset maynot include all products containing carcinogens or developmental or reproductive toxicants due to companies failing to report.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis: First Presidential Debate 2016</title>
      <link>https://yanfei-wu.github.io/blog/2016-09-30_debate/</link>
      <pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yanfei-wu.github.io/blog/2016-09-30_debate/</guid>
      <description>

&lt;p&gt;The US presidential election of 2016 is in less than 2 months. After series of presidential primary elections and caucuses, businessman Donald Trump became the Republican Party&amp;rsquo;s presidential nominee and former Secretary of State Hillary Clinton became the Democratic Party&amp;rsquo;s presidential nominee. Before November, there are three presidential debates between Clinton and Trump. The first debate just took place several days ago on September 26th.&lt;/p&gt;

&lt;p&gt;In this post, I show some results of the sentiment analysis I did using tweets collected after the first debate. All codes for this post can be found in &lt;a href=&#34;https://github.com/yanfei-wu/presidential_debate/tree/master/first_debate&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;getting-data-from-twitter&#34;&gt;Getting Data From Twitter&lt;/h3&gt;

&lt;p&gt;20,000 tweets were queried using the two candidates&amp;rsquo; names (10,000 tweets for each candidate) from Twitter. The &lt;code&gt;twitteR&lt;/code&gt; package was used and a brief user guide can be find &lt;a href=&#34;https://cran.r-project.org/web/packages/twitteR/README.html&#34;&gt;here&lt;/a&gt;. An additional filtering step was carried out to remove tweets containing both names just to simplify the assignment of the sentiment scores to each candidate. (7179 tweets were left only mentioning Clinton&amp;rsquo;s name and 8854 tweets only mentioning Trump&amp;rsquo;s name.)&lt;/p&gt;

&lt;h3 id=&#34;sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/h3&gt;

&lt;p&gt;The sentiment scores were calculated using a &lt;a href=&#34;https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf&#34;&gt;lexicon-based method&lt;/a&gt; as proposed by Hu and Liu. A list of English positive and negative opinion words or sentiment words compiled by them were used (you can find it &lt;a href=&#34;https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon&#34;&gt;here&lt;/a&gt;). The different between the number of positive words and the number of negative words in each tweet was used to determine the option orientation or the sentiment score of each tweet.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Before calculating the sentiment scores, the tweets were cleaned by removing punctuations, special characters (@ and #), and URLs.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s look at the distribution of sentiment scores for each candidate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/histogram-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/boxplot-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Both candidates have similar variance in terms of their sentiment score distributions with a similar median roughly around 0. And both have outliers or extreme scores (positive and negative). Overall, it is interesting that based on these tweets &lt;strong&gt;people seem to be less happy with Clinton than with Trump&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The raw sentiment scores were then used to divide the sentiment into 3 categories below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Positive(1): &lt;code&gt;sentiment score &amp;gt;= 2&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Neutral(0): &lt;code&gt;-2 &amp;lt; sentiment score &amp;lt; 2&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Negative(-1): &lt;code&gt;sentiment score &amp;lt;= -2&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The distributions of positive, neutral, and negative sentiments for each candidate are shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/barplot.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Again, the majority (about 80%) of tweets are neutral towards both candidates but there are indeed bias! &lt;strong&gt;More tweets are negative about Clinton than those that are positive&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;word-frequency-analysis&#34;&gt;Word Frequency Analysis&lt;/h3&gt;

&lt;p&gt;Another interesting thing we can do with tweets is to a word frequency analysis. The first step is to construct corpus by compiling the tweets. Then, the texts in the corpus are broken into tokens, i.e., single word. After tokenization, document-term matrix is created to describe the frequency of each word that occurs in the corpus. Note that some words are removed from this analysis such as the candidates&amp;rsquo; names and English stopwords.&lt;/p&gt;

&lt;p&gt;The frequency plots are shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/word.frequency.plot-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/word.cloud.clinton-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/word.cloud.trump-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Not surprisingly, words like &amp;ldquo;Bill&amp;rdquo;, &amp;ldquo;Emails&amp;rdquo;, &amp;ldquo;FBI&amp;rdquo; appear in tweets about Clinton quite frequently. It also looks like a lot of tweets about Clinton also mention Gary Johnson. In tweets about Trump, some frequent words are &amp;ldquo;race&amp;rdquo;, &amp;ldquo;Obama&amp;rdquo;, and also words like &amp;ldquo;unfit&amp;rdquo;!&lt;/p&gt;

&lt;h3 id=&#34;a-final-note&#34;&gt;A Final Note&lt;/h3&gt;

&lt;p&gt;The analysis shown here is just a sneak peek of the sentiment towards the two candidates. A more thorough analysis could be done with more data (more tweets or incorporating data from other sources), or more complex sentiment metrics. It would also be interesting to dig into those biased data (positive or negative sentiment).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello World!</title>
      <link>https://yanfei-wu.github.io/blog/2016-08-26_hello/</link>
      <pubDate>Fri, 26 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yanfei-wu.github.io/blog/2016-08-26_hello/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;About six months ago I was still working on my thesis. Putting together everything I did over the past six years brought me back to those days in graduate school. It reminded me of the ups and downs along the way, and of course the six freezing winters I spent in Minnesota&amp;hellip;It was definitely not an easy six year in my life but I was grateful that I had such an experience. I appreciated all the challenges I had to take, all the cool things I learned, and all the wonderful people I met. These are things that made me stronger and they prepared me for my new adventure - data science.&lt;/p&gt;

&lt;p&gt;About four months ago, I started to learn data science from MOOCs, books, and some wonderful blogs. It has been a mixture of fun and challenges (and a lot of screen time!). With this site, I hope to gear my learning process towards hands-on.&lt;/p&gt;

&lt;p&gt;Finally, something fun to share - a word cloud of my thesis:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../hello_file/thesis_wordcloud.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>