<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing on Yanfei Wu</title>
    <link>/tags/natural-language-processing/index.xml</link>
    <description>Recent content in Natural Language Processing on Yanfei Wu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2016 Yanfei Wu</copyright>
    <atom:link href="/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>South Park Lines: Who is Speaking? (Part 1)</title>
      <link>/blog/2017-03-20_southpark/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-03-20_southpark/</guid>
      <description>

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;South Park is an adult animated sitcom about four potty-mouthed grade boys Eric Cartman, Stan Marsh, Kyle Broflovshi, and Kenny McCormick and their adventures in a fictional Colorado town called South Park. The show debuted in 1997 and has been through 20 seasons now. It is such a famous show and it has won a lot of awards but for some reason I never really watched this show (except for a couple YouTube clips).&lt;/p&gt;

&lt;p&gt;Recently, I found a dataset from &lt;a href=&#34;https://www.kaggle.com/tovarischsukhov/southparklines&#34;&gt;Kaggle&lt;/a&gt; with the lines of the first 18 seasons. These lines have been annotated with season, episode and speaker. I was curious about what I can find out about the show just from this dataset. Also, I thought it would be interesting to build a classification model to guess the speakers based on the lines. This post will focus on the first part using data visualization. All the codes for this post can be found in &lt;a href=&#34;https://github.com/yanfei-wu/tv_lines&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here it goes.&lt;/p&gt;

&lt;h3 id=&#34;get-to-know-the-show&#34;&gt;Get to Know the Show&lt;/h3&gt;

&lt;p&gt;First, I looked at some basic information about the show based on this dataset, such as the number of episodes, the number of characters, the number of lines, etc. The information is summarized in the table below:&lt;/p&gt;

&lt;div&gt;
&lt;table border=&#34;1&#34; align=&#34;center&#34;&gt;
  &lt;thead&gt;
    &lt;tr align=&#34;center&#34;&gt;
      &lt;th&gt; Seasons &lt;/th&gt;
      &lt;th&gt;  Episodes  &lt;/th&gt;
      &lt;th&gt;  Characters  &lt;/th&gt;
      &lt;th&gt;  Lines  &lt;/th&gt;
      &lt;th&gt;  Words  &lt;/th&gt;
      &lt;th&gt;  Words (Unique)  &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr align=&#34;center&#34;&gt;
      &lt;td width=&#34;15%&#34;&gt; 18 &lt;/td&gt;
      &lt;td width=&#34;15%&#34;&gt; 257 &lt;/td&gt;
      &lt;td width=&#34;15%&#34;&gt; 3,949 &lt;/td&gt;
      &lt;td width=&#34;15%&#34;&gt; 70,789 &lt;/td&gt;
      &lt;td width=&#34;15%&#34;&gt; 811,296 &lt;/td&gt;
      &lt;td width=&#34;20%&#34;&gt; 30,347 &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;  

&lt;p&gt;So how long are the lines? The number of words in a line is used to define the line length. The distribution of the line length shows that the majority of the lines in the show are relatively short (with a median of 8 words and a 75 percentile of 14 words).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/linelength_dist.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Another question is how did the show evolve over the seasons in terms of the number of episodes, characters, and lines?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/over_season.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Based on the plot above, the show started off with 13 episodes and around 250 characters in Season 1. On average, it had about 320 lines per episode but the lines seem to be short. The first season was clearly a success as we can see a big jump in the numbers of episodes and characters as well as the average numbers of lines and words in Season 2. Overall, these numbers peaked from Season 2 through Season 4 followed by some slow decay. During Season 10 to Season 15, more characters were included in each season although each season remained to have 14 episodes. The average number of lines, interestingly, did not show a corresponding increase. But the lines were definitely getting longer on average from Season 13.&lt;/p&gt;

&lt;h3 id=&#34;get-to-know-the-characters&#34;&gt;Get to Know the Characters&lt;/h3&gt;

&lt;h4 id=&#34;who-spoke-the-most&#34;&gt;Who Spoke the Most?&lt;/h4&gt;

&lt;p&gt;We know that the story is about the four boys - Cartman, Stan, Kyle, and Kenny. Let&amp;rsquo;s see if that is what the lines are telling us. But first, we can see who actually appeared in all 18 seasons. It turned out that they are: &lt;em&gt;Stan, Kyle, Cartman, Gerald, Mr. Mackey, Randy, Kenny, Clyde, Sheila, Sharon, Wendy, Principal Victoria, and Liane (and also voices annotated as &amp;lsquo;Woman&amp;rsquo;, &amp;lsquo;Man&amp;rsquo;, and &amp;lsquo;Announcer&amp;rsquo;)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Then, let&amp;rsquo;s see who spoke the most. We can check both the total number of lines and the number of words for each character.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/topspeaker.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly, Cartman is No. 1, followed by Stan and Kyle. Kenny, however, falls out of the top 5. Instead, Butters and Randy are in the top 5 list although their line shares are significantly less than the other three. Also something to note is Butters actually did not appear in all 18 seasons. He was introduced into the show in Season 2.&lt;/p&gt;

&lt;p&gt;So from now on, we can just focus on the top 5 speakers. The changes of their word shares over the seasons are shown below. Cartman remained to be No. 1 throughout the seasons, taking up to around 15% of the word shares. Stan and Kyle started off similarly like Cartman but their word shares decreased over time. Butters and Randy did not speak much in the first 4 seasons but they rise to be among the top speakers and ended up having similar word shares as Stan and Kyle, around 5%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/wordshare.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/linelength.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The average lengths of the lines (defined as the total number of words divided by total number of lines) for Cartman, Stan, Kyle and Randy have not changed dramatically over the seasons. Also, their line lengths are relatively close, 10-15 words per line. But for Butters, his average line length shows a huge jump from Season 2 (when he was first introduced to the show) to season 3. I was very curious about exactly how this character was introduced. What did he say in Season 2? It turned out that he only said two lines:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Me, too!&amp;rdquo;&lt;br /&gt;
 &amp;ldquo;Pass this up.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;what-did-top-speakers-say&#34;&gt;What did Top Speakers Say?&lt;/h4&gt;

&lt;p&gt;So what exactly did the characters say? A word cloud from &lt;em&gt;all the lines&lt;/em&gt; is shown as the cover image of this post. But I am more interested in the &lt;em&gt;lines of the top speakers&lt;/em&gt;. What are their most frequent words and the most characteristic words?&lt;/p&gt;

&lt;p&gt;Finding out the most frequent words is simply a word count problem. A note here is that stopwords were removed. Below I show the top 20 most frequent words for our top 5 speakers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/frequent_word.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Compared to knowing the most frequent words, finding out the most characteristic words for each speaker is probably more interesting. Here, &lt;strong&gt;term frequency-inverse document frequency&lt;/strong&gt; or &lt;strong&gt;tf-idf&lt;/strong&gt; is used. It is often used in information retrieval and text mining as a measure to evaluate how important a word is to a document in a corpus. The tf-idf weight is composed by 2 components. one is the term frequency (tf) and it is simply the number of times term t appears in a document divided by the total number of terms in the corpus. The other component is the inverse document frequency (idf) and it is the natural log of the ratio between the total number of documents and the number of documents with term t in it. The final tf-idf weight is calculated as the product between tf and idf. So a term is more important to a document if it appears in this document for many times (large tf) but it does not appear in many of the documents in the corpus (large idf).&lt;/p&gt;

&lt;p&gt;For our top 5 speakers, their most important words and phrases are shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/important_word.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/important_phrase.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There are some swear words, as you can see. And Kenny apparently had been killed many times!&lt;/p&gt;

&lt;h4 id=&#34;what-did-kenny-say&#34;&gt;What did Kenny Say?&lt;/h4&gt;

&lt;p&gt;As one of the four main characters, Kenny had significantly small word shares in the show. So who is Kenny? What did he do in the story? Do these lines tell anything about what he is like? We can simply construct a word cloud using Kenny&amp;rsquo;s lines in the dataset (with stopwords removed). Apparently, the F word is among the top of Kenny&amp;rsquo;s vocabulary, along with other swear words!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/kenny_wordcloud.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The word cloud is based on word frequency. Another way to look at this is the importance of the words. The most important 20 words in Kenny&amp;rsquo;s lines with the highest tf-idf weights are shown below. We know that the show is known for its use of profanity. It seems that Kenny, one of the main characters in the show who spoke the least mainly spoke profanity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../southpark_file/topwords_kenny.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;In this post, I walked you through my exploration of the South Park lines dataset and showed you what I learned about the show and the characters from this dataset. My next step is to build a classification model which is able to guess the speaker based on the lines given. Stay tuned :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis: First Presidential Debate 2016</title>
      <link>/blog/2016-09-30_debate/</link>
      <pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-09-30_debate/</guid>
      <description>

&lt;p&gt;The US presidential election of 2016 is in less than 2 months. After series of presidential primary elections and caucuses, businessman Donald Trump became the Republican Party&amp;rsquo;s presidential nominee and former Secretary of State Hillary Clinton became the Democratic Party&amp;rsquo;s presidential nominee. Before November, there are three presidential debates between Clinton and Trump. The first debate just took place several days ago on September 26th.&lt;/p&gt;

&lt;p&gt;In this post, I show some results of the sentiment analysis I did using tweets collected after the first debate. All codes for this post can be found in &lt;a href=&#34;https://github.com/yanfei-wu/presidential_debate/tree/master/first_debate&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;getting-data-from-twitter&#34;&gt;Getting Data From Twitter&lt;/h3&gt;

&lt;p&gt;20,000 tweets were queried using the two candidates&amp;rsquo; names (10,000 tweets for each candidate) from Twitter. The &lt;code&gt;twitteR&lt;/code&gt; package was used and a brief user guide can be find &lt;a href=&#34;https://cran.r-project.org/web/packages/twitteR/README.html&#34;&gt;here&lt;/a&gt;. An additional filtering step was carried out to remove tweets containing both names just to simplify the assignment of the sentiment scores to each candidate. (7179 tweets were left only mentioning Clinton&amp;rsquo;s name and 8854 tweets only mentioning Trump&amp;rsquo;s name.)&lt;/p&gt;

&lt;h3 id=&#34;sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/h3&gt;

&lt;p&gt;The sentiment scores were calculated using a &lt;a href=&#34;https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf&#34;&gt;lexicon-based method&lt;/a&gt; as proposed by Hu and Liu. A list of English positive and negative opinion words or sentiment words compiled by them were used (you can find it &lt;a href=&#34;https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon&#34;&gt;here&lt;/a&gt;). The different between the number of positive words and the number of negative words in each tweet was used to determine the option orientation or the sentiment score of each tweet.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Before calculating the sentiment scores, the tweets were cleaned by removing punctuations, special characters (@ and #), and URLs.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s look at the distribution of sentiment scores for each candidate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/histogram-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/boxplot-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Both candidates have similar variance in terms of their sentiment score distributions with a similar median roughly around 0. And both have outliers or extreme scores (positive and negative). Overall, it is interesting that based on these tweets &lt;strong&gt;people seem to be less happy with Clinton than with Trump&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The raw sentiment scores were then used to divide the sentiment into 3 categories below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Positive(1): &lt;code&gt;sentiment score &amp;gt;= 2&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Neutral(0): &lt;code&gt;-2 &amp;lt; sentiment score &amp;lt; 2&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Negative(-1): &lt;code&gt;sentiment score &amp;lt;= -2&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The distributions of positive, neutral, and negative sentiments for each candidate are shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/barplot.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Again, the majority (about 80%) of tweets are neutral towards both candidates but there are indeed bias! &lt;strong&gt;More tweets are negative about Clinton than those that are positive&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;word-frequency-analysis&#34;&gt;Word Frequency Analysis&lt;/h3&gt;

&lt;p&gt;Another interesting thing we can do with tweets is to a word frequency analysis. The first step is to construct corpus by compiling the tweets. Then, the texts in the corpus are broken into tokens, i.e., single word. After tokenization, document-term matrix is created to describe the frequency of each word that occurs in the corpus. Note that some words are removed from this analysis such as the candidates&amp;rsquo; names and English stopwords.&lt;/p&gt;

&lt;p&gt;The frequency plots are shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/word.frequency.plot-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/word.cloud.clinton-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../debate_file/word.cloud.trump-1.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Not surprisingly, words like &amp;ldquo;Bill&amp;rdquo;, &amp;ldquo;Emails&amp;rdquo;, &amp;ldquo;FBI&amp;rdquo; appear in tweets about Clinton quite frequently. It also looks like a lot of tweets about Clinton also mention Gary Johnson. In tweets about Trump, some frequent words are &amp;ldquo;race&amp;rdquo;, &amp;ldquo;Obama&amp;rdquo;, and also words like &amp;ldquo;unfit&amp;rdquo;!&lt;/p&gt;

&lt;h3 id=&#34;a-final-note&#34;&gt;A Final Note&lt;/h3&gt;

&lt;p&gt;The analysis shown here is just a sneak peek of the sentiment towards the two candidates. A more thorough analysis could be done with more data (more tweets or incorporating data from other sources), or more complex sentiment metrics. It would also be interesting to dig into those biased data (positive or negative sentiment).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>