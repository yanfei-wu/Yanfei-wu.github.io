<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Yanfei Wu</title>
    <link>/tags/machine-learning/index.xml</link>
    <description>Recent content in Machine Learning on Yanfei Wu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2016 Yanfei Wu</copyright>
    <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Fraud Detection: Learning from Imbalanced Data</title>
      <link>/blog/2017-06-20_fraud_detection/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-06-20_fraud_detection/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The code and full report for this project is available in &lt;a href=&#34;https://github.com/yanfei-wu/kaggle/tree/master/fraud&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Combating fraud is an important application of Machine Learning. Fraud exists in many industries including banking and financial sectors, insurance, government agencies, and more. With the advances in computer technology and e-commerce, fraud attempts have seen a drastic increase in recent years, making fraud detection more important than ever. Machine learning for fraud detection works on the basis of large, historical datasets that have been aggregated across many clients. The data serve as the training set and allow businesses to build efficient machine-learning-based fraud detection systems with optimized performance. Compared to standard predictive modeling problem, fraud detection is characterized with skewed class distribution, or class imbalance, since fraud is a relatively rare event. Therefore, one challenge of fraud detection with Machine Learning is to mitigate the class imbalance problem.&lt;/p&gt;

&lt;p&gt;The goal of this project is to build supervised classification models for fraud detection using preprocessed and anonymized credit card transaction data. The main focus will be on dealing with class imbalance.&lt;/p&gt;

&lt;h2 id=&#34;2-analysis&#34;&gt;2. Analysis&lt;/h2&gt;

&lt;h3 id=&#34;2-1-data&#34;&gt;2.1 Data&lt;/h3&gt;

&lt;p&gt;The dataset (available in &lt;a href=&#34;https://www.kaggle.com/dalpozz/creditcardfraud&#34;&gt;Kaggle&lt;/a&gt; contains transactions made by credit cards in September 2013 by European cardholders. It contains only numerical input variables, including 28 features which are the result of a PCA transformation, and 1 feature which has not been transformed with PCA, namely &lt;code&gt;&#39;Amount&#39;&lt;/code&gt;. There are no missing values in the dataset. The original features and more background information about the data are not available due to confidentiality issues. The target variable for the dataset is &lt;code&gt;&#39;Class&#39;&lt;/code&gt;. It takes value 1 in case of fraud and 0 otherwise. This dataset presents transactions that occurred in two days, where there are 492 frauds out of 284,807 transactions. &lt;strong&gt;Clearly, the dataset is highly skewed, the positive class (frauds) only account for 0.172% of all transactions.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-2-methodology&#34;&gt;2.2 Methodology&lt;/h3&gt;

&lt;p&gt;Class imbalance exists in many real-world classification problems and handling imbalanced dataset has been an actively studied subject in Machine Learning. Before digging into the current problem, some common approaches for dealing with imbalanced data is reviewed below. These approaches include:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Collect more data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Having more data might lead to a more balanced dataset. A larger dataset is also helpful for minority class resampling. However, this approach is not always possible in real life.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Balance the training set.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This can be done by resampling the training set or generating synthetic samples.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Oversample the minority class.&lt;/em&gt; Oversampling randomly replicates minority instances to increase their population.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Undersample the majority class.&lt;/em&gt; Undersampling randomly downsamples the majority class.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Synthesize new minority classes.&lt;/em&gt; The best-known example of this approach is SMOTE (Synthetic Minority Oversampling TEchnique). It creates synthetic samples from the minority class instead of making duplicates. The algorithm selects two or more similar instances, i.e., nearest neighbors (using a distance measure), and perturbs an instance one attribute at a time by a random amount within the difference to the neighboring instances to construct new minority instances.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Resampling is the easiest to implement. However, both under/over-sampling have their drawbacks. Undersampling discards potentially important data and makes the independent variables look more variable than they actually are; whereas data duplication with oversampling makes the variable appear to have low variance and could lead to overfitting. SMOTE, compared to traditional oversampling, creates similar examples instead of exact copies of the minority class, which results in more general models and avoids overfitting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Change the performance metrics.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The performance of classification algorithms is typically evaluated by a confusion matrix which compares the predicted class and the actual class. In the confusion matrix, the number of negative examples correctly classified is True Negatives (TN), the number of negative examples incorrectly classified as positive is False Positives (FP), the number of positive examples incorrectly classified as negative is False Negatives (FN) and the number of positive examples correctly classified is True Positives (TP). In the context of balanced datasets and equal error costs, the common performance metric is predictive accuracy, which is defined as the percentage of correctly identified examples, i.e., (TP+TN)/(TP+FP+TN+FN). In the case of imbalanced datasets, it is more appropriate to use other metrics.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Precision-recall curve.&lt;/em&gt; Precision is defined as TP/(TP+FP) and is a measure of the classifier&amp;rsquo;s exactness. Recall, on the other hand, is defined as TP/(TP+FN). It measures the classifier&amp;rsquo;s sensitivity. There is a tradeoff between precision and recall. For a given model, the classifier performance can be visualized using a precision-recall curve, which captures the tradeoff between precision and recall as the decision threshold is varied. On its own, a model is overall better if the curve is closer to the top right of the chart where precision and recall are both 1.0. One way to capture the overall quality of the model is by computing the area under the curve (AUC). The AUC will be 1.0 for an ideal model.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that Receiver Operator Characteristic (ROC) curve is also commonly used to measure the performance of binary classifiers. However, when dealing with highly skewed datasets, it provides a less informative picture of the algorithm&amp;rsquo;s performance than the precision-recall curve.[1]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Algorithm level approach.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Adjust the class weight.&lt;/em&gt; This approach is based on reducing the misclassification costs for minority class. Many Machine Learning toolkits have ways to adjust the importance of the classes. For examples, many classifiers in &lt;code&gt;Scikit-learn&lt;/code&gt; have an optional &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; parameter that can be set to &amp;lsquo;balanced&amp;rsquo;.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Adjust the decision threshold.&lt;/em&gt; Instead of getting classification labels from the classifier, a good approach to handling class imbalance is to estimate probability and then adjust the decision threshold to separate classes. Probabilistic classification models in &lt;code&gt;Scikit-learn&lt;/code&gt; such as logistic regression offer &lt;code&gt;&#39;predict_proba&#39;&lt;/code&gt; method to estimate probability.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-results-and-discussions&#34;&gt;3. Results and Discussions&lt;/h2&gt;

&lt;h3 id=&#34;3-1-model-selection-and-effect-of-adjusting-class-weight&#34;&gt;3.1 Model Selection and Effect of Adjusting Class Weight&lt;/h3&gt;

&lt;p&gt;As mentioned above, for classification problem with skewed data, it is better to estimate probability than labels. So several naturally probabilistic classification algorithms are selected here, including Naive Bayes, Logistic Regression, Decision Tree Classifier, and Random Forest. Also, except for Naive Bayes, all other models have the &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; parameter that can be set to &amp;lsquo;balanced&amp;rsquo;. So before any resampling of the data, the performance of these models (with or without setting &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; to be &amp;lsquo;balanced&amp;rsquo;) is compared.&lt;/p&gt;

&lt;p&gt;The dataset is first randomly split into 80% training set and 20% test set. The features are normalized before training the models. The precision-recall curves of different models (Figure 2) and their AUC scores (Figure 3) are shown below. Since a model is considered better if the curve is closer to the top right of the chart, in this particular case, Random Forest model with &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; set to be &amp;lsquo;balanced&amp;rsquo; seems to be the best. The comparison of areas under the precision-recall curves also shows that Random Forest model (balanced) gives the largest area followed by Random Forest model (not balanced), and logistic model (not balanced). Interestingly, although adjusting class weight shows a positive effect on Random Forest model, it does not result in significant change of the model performance for Logistic Regression, and it even exerts a negative effect (worse precision-recall performance) for Decision Tree Classifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fraud_file/pr.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fraud_file/auc.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-2-effect-of-smote&#34;&gt;3.2 Effect of SMOTE&lt;/h3&gt;

&lt;p&gt;An alternative to adjusting class weight at the algorithm level is to balance the training set, for example, with SMOTE. As introduced above, SMOTE is an oversampling technique, but instead of creating exact copies of the existing minority data, SMOTE generates new, synthetic minority points by interpolating between current minority examples.&lt;/p&gt;

&lt;p&gt;A Python package named &lt;a href=&#34;http://contrib.scikit-learn.org/imbalanced-learn/index.html#&#34;&gt;&lt;code&gt;&#39;imbalanced-learn&#39;&lt;/code&gt;&lt;/a&gt; is used to implement SMOTE. The package offers a number of re-sampling techniques commonly used in datasets showing strong between-class imbalance. SMOTE operation generates additional positive examples so that the class ratio of the training set becomes 1:1. The above four classification algorithms, Naive Bayes, Logistic Regression, Decision Tree, and Random Forest (all out-of-box and without setting &lt;code&gt;&#39;class_weight&#39;&lt;/code&gt; to &amp;lsquo;balanced&amp;rsquo;) are trained using the new balanced training set. The models are then compared with their precision-recall curves (Figure 4) and AUC scores (Figure 5). The best model is again out-of-box Random Forest model. The AUC score of the Random Forest model after SMOTE (0.852) is better than the one without SMOTE (0.849), but is not as good as Random Forest model with adjusting class weight (0.861). In other words, both approaches improve the performance of the model but in this particular case, adjusting the class weight is slightly better than SMOTE. Note that SMOTE is limited in the fact that it can only generate examples WITHIN the body of available examples, i.e., it can only fill in the convex hull of existing minority examples but not outside.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fraud_file/pr_smote.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fraud_file/auc_smote.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-conclusion&#34;&gt;4. Conclusion&lt;/h2&gt;

&lt;p&gt;This project explores approaches for handling class imbalance by building supervised fraud detection models using credit card transaction data. The original dataset is highly skewed with the positive class only accounting for 0.172%. Therefore, instead of using common metrics like accuracy, the precision-recall curve and the area under the curve (AUC) are used as the performance measure for the models. Algorithm level approach (i.e., adjusting class weight) and data rebalancing technique (SMOTE) are used to optimize the model performance. It is found that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Both adjusting class weight and SMOTE result in better model performance as measured by the precision-recall curve.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Random Forest model turns out to be the best performing model compared to Naive Bayes, Logistic Regression, and Decision Tree Classifier.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Adjusting class weight of Random Forest model gives slightly better result than training the model on SMOTE-balanced dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;[1] J. Davis et al. The relationship between Precision-Recall and ROC curves. Proceedings of the 23rd international conference on Machine learning, 2006.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Employee Retention</title>
      <link>/blog/2017-04-28_employee_retention/</link>
      <pubDate>Fri, 28 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-04-28_employee_retention/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The code and full report for this project is available in &lt;a href=&#34;https://github.com/yanfei-wu/ml_udacity/tree/master/capstone&#34;&gt;my Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h3&gt;

&lt;p&gt;Recruiting excellent employees is one thing, but keeping them is another. While losing employees who are poor performers can have positive effects, high employee turnover rate is generally regarded as bad for the business. It increases expenses since the process of identifying, hiring and training employees is very expensive. Studies have shown that cost related to directly replacing an employee can be as high as 50–60% of the employee’s annual salary, and the total cost of turnover can reach as high as 90–200% of the employee’s annual salary.[1] Even worse, frequent employee turnover can destroy the company morale, resulting in decreased performance in the workplace. Therefore, retaining its valuable and talented employees is vital to a company&amp;rsquo;s success.&lt;/p&gt;

&lt;p&gt;A novel approach to implementing an effective retention program and preventing key workers from leaving prematurely is to use machine learning techniques. For example, a supervised classification model can be trained on a dataset containing features related to the employees and whether they left the company (a dataset that is available to many companies). Building such a model provides insights to key factors that result in employee turnover. It also allows the management and the human resource team to predict which employee is going to leave so that they could intervene immediately.&lt;/p&gt;

&lt;p&gt;The goal of this project is to build such a supervised binary classification model. A simulated dataset (available in &lt;a href=&#34;https://www.kaggle.com/ludobenistant/hr-analytics&#34;&gt;Kaggle&lt;/a&gt;) containing a series of employee-related features and a binary class label of whether the employee left or not is used. The expected outcome of this project is to help the management and the human resource team:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;predict which current employee is going to leave (class label) so that they could intervene immediately;&lt;/li&gt;
&lt;li&gt;identify which are the most important factors (features) that lead to employee turnover so that changes can be implemented to ensure employees remain in place while maintaining high work performance and productivity.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;2-datasets-and-exploratory-visualization&#34;&gt;2. Datasets and Exploratory Visualization&lt;/h3&gt;

&lt;p&gt;The dataset contains 14,999 rows and 10 columns. There are both numerical and categorical features in this dataset, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;satisfaction_level&#39;&lt;/code&gt;: employee satisfaction level (numerical, float numbers between 0 and 1)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;last_evaluation&#39;&lt;/code&gt;: the score from the employee&amp;rsquo;s last evaluation (numerical, float numbers between 0 and 1)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;number_project&#39;&lt;/code&gt;: the number of projects the employee worked on (numerical, integers)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;average_monthly_hours&#39;&lt;/code&gt;: the average monthly hours the employee spent on work (numerical, integers)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;time_spend_company&#39;&lt;/code&gt;: number of year the employee spent at the company (numerical, integers)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;work_accident&#39;&lt;/code&gt;: whether the employee had a work accident (categorical, &amp;lsquo;0&amp;rsquo; - no, &amp;lsquo;1&amp;rsquo; - yes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;promotion_last_5year&#39;&lt;/code&gt;: whether the employee had a promotion in the last 5 years (categorical, &amp;lsquo;0&amp;rsquo; - no, &amp;lsquo;1&amp;rsquo; - yes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;sales&#39;&lt;/code&gt;: the Department the employee works (categorical, &amp;lsquo;sales&amp;rsquo;, &amp;lsquo;accounting&amp;rsquo;, &amp;lsquo;hr&amp;rsquo;, &amp;lsquo;technical&amp;rsquo;, &amp;lsquo;support&amp;rsquo;, &amp;lsquo;management&amp;rsquo;, &amp;lsquo;IT&amp;rsquo;, &amp;lsquo;product_mng&amp;rsquo;, &amp;lsquo;marketing&amp;rsquo;, &amp;lsquo;RandD&amp;rsquo;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;salary&#39;&lt;/code&gt;: the salary of the employee (categorical, &amp;lsquo;low&amp;rsquo;, &amp;lsquo;medium&amp;rsquo;, &amp;lsquo;high&amp;rsquo;)&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The target variable in the dataset is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;left&#39;&lt;/code&gt;: whether the employee has left or not (categorical, &amp;lsquo;0&amp;rsquo; - no, &amp;lsquo;1&amp;rsquo; - yes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Note that the number of employees who left in the dataset is 3571 and is 23.81% of the total employees (14,999).&lt;/em&gt; As discussed above, with the unbalanced class distributions, care must be taken when choosing performance measure for the models. F2 score will be used in order to weight recall higher than precision.&lt;/p&gt;

&lt;p&gt;Also note is that there are no &lt;code&gt;NaN&lt;/code&gt; values and all the numerical values are within reasonable range. No outliers are detected.&lt;/p&gt;

&lt;p&gt;Both the numerical and the categorical features are visualized below.&lt;br /&gt;
&lt;img src=&#34;../employee_file/numerical.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Some observations from the numerical features are&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;time_spend_company&lt;/code&gt; and &lt;code&gt;number_project&lt;/code&gt; take integer values. This dataset only considers employees at the company for 2 or more years with 2 or more projects.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;average_monthly_hours&#39;&lt;/code&gt;, &lt;code&gt;&#39;last_evaluation&#39;&lt;/code&gt;, and &lt;code&gt;&#39;satisfaction_level&#39;&lt;/code&gt; take continuous values. All of them show bimodal distributions.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;../employee_file/categorical.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Some Observations from the categorical features are&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;work_accident&#39;&lt;/code&gt; and &lt;code&gt;&#39;promotion_last_5years&#39;&lt;/code&gt; are two-level categorical variables. Both show very unbalanced distribution (with the majority being category 0: no work accident or no promotion in last 5 years)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;salary&#39;&lt;/code&gt; has 3 levels: low, medium and high. The salary structure seems to reflect the reality, with the majority of employees having low and medium salaries.&lt;/li&gt;
&lt;li&gt;Employees from 10 different departments are included in the dataset, with the majority of employees from sales, technical and support. This also somewhat reflects the reality in many companies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A correlation matrix of the dataset (excluding &lt;code&gt;&#39;department&#39;&lt;/code&gt;) is shown below.&lt;br /&gt;
&lt;img src=&#34;../employee_file/correlationmatrix.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The correlation matrix shows that:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There is a strong negative correlation between &lt;code&gt;&#39;satisfaction_level&#39;&lt;/code&gt; and &lt;code&gt;&#39;left&#39;&lt;/code&gt; with a correlation coefficient of -0.39.&lt;/li&gt;
&lt;li&gt;Variables &lt;code&gt;&#39;number_project&#39;&lt;/code&gt;, &lt;code&gt;&#39;average_monthly_hours&#39;&lt;/code&gt;, and &lt;code&gt;&#39;last_evaluation&#39;&lt;/code&gt; show strong positive correlations, which makes sense intuitively.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;4-models&#34;&gt;4. Models&lt;/h3&gt;

&lt;h4 id=&#34;4-1-algorithms-and-techniques&#34;&gt;4.1 Algorithms and Techniques&lt;/h4&gt;

&lt;p&gt;It is a binary classification problem and there are a number of algorithms available in the &lt;code&gt;scikit-learn&lt;/code&gt; library. To find the best model for the problem, 6 different algirithms are used to train the dataset, including Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbors, Decision Trees, Support Vector Machines, and Random Forest. All the algorithms are evaluated out-of-box, i.e., default hyperparameters will be used. A naive benchmark model is also built to set a baseline for model performance. The benchmark model is to predict that all the employees are leaving. In other words, the management and HR team treat every single employee as if he/she is leaving. Note that implementing such a naive classifier will probably lower the employee turnover rate but most of the time, it is not a realistic retention program for most companies. So, it is only for comparison purpose here.&lt;/p&gt;

&lt;p&gt;Typically, the performance of a classification model can be measured by accuracy, i.e., the percentage of correct predictions. But in this case, it is not the best metric due to the class imbalance (~25% class &amp;lsquo;left&amp;rsquo; vs ~75% class &amp;lsquo;stay&amp;rsquo;). Instead, we can use a combination of precision and recall as an unambiguous way to show the prediction results. In this specific problem, we would like to avoid predicting valuable employees who are actually leaving as staying and thus taking no actions. Therefore, the so-called F2 score is used. It is a weighted average of precision and recall and it places more emphasis on recall (minimizing False Negatives).&lt;/p&gt;

&lt;p&gt;Note that before the dataset can be used to train the classificatin models, some preprocessing steps are done, including one-hot encoding of the categorical features and normalization of the numerical features. The latter ensures that each feature is treated equally when applying supervised learners.&lt;/p&gt;

&lt;h4 id=&#34;4-2-model-selection-and-feature-importance&#34;&gt;4.2 Model Selection and Feature Importance&lt;/h4&gt;

&lt;p&gt;In order to evaluate each algorithm, a 10-fold cross validation procedure is carried out. The cross validation F2 scores for the 6 models are shown in the boxplot below. The F2 score of the benchmark model is also plotted as a baseline.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../employee_file/modelcomparison.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Based on the comparison, Decision Trees classifier and Random Forest both perform very well. They give much better F2 score than the benchmark classifier. Generally, Random Forest algorithm is less likely to overfit the data compared to Decision Trees. So in this case, it is chosen as the best model.&lt;/p&gt;

&lt;p&gt;The Random Forest model is further tuned with &lt;code&gt;GridSearchCV&lt;/code&gt;. The best 10-fold cross validation F2 score is 0.9754, which shows slight improvevment than the score from the out-of-box Ramdom Forest model (0.9660).&lt;/p&gt;

&lt;p&gt;With a classification model like the Random Forest classifier built above, management and human resource team are now able to predict if a current employee is leaving so that they can take immediate internvention. Another key information for an effective employee retention program is the knowledge of the most important factors on employee turnover. Luckily, with Random Forest classifier, we can easily obtain the feature importance from a trained model. A barplot of the top 5 features from the optimized Random Forest model trained on the entire training set is shown below.&lt;/p&gt;

&lt;p&gt;Thses 5 features consists about &lt;strong&gt;95%&lt;/strong&gt; of the total feature importance. &lt;code&gt;&#39;satisfaction_level&#39;&lt;/code&gt; is determined to be the most important feature, which is consistant with its high correlation coefficient with the target variable &lt;code&gt;&#39;left&#39;&lt;/code&gt; as shown in the correlation matrix. The other features highly relevant to employee turnover are &lt;code&gt;&#39;average_monthly_hours&#39;&lt;/code&gt;, &lt;code&gt;&#39;number_project&#39;&lt;/code&gt;, &lt;code&gt;&#39;time_spend_company&#39;&lt;/code&gt;, and &lt;code&gt;&#39;last_evaluation&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../employee_file/featureimportance.png&#34; class=&#34;img-responsive&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h3&gt;

&lt;p&gt;To summarize, this project outlines a machine learning approach to help implement an effective employee retention program. A synthesized dataset containing both numerical and categorical features of the employees, as well as a binary label of whether the employee left or not is used to build a binary classification model. The categorical features are transformed to numerical values by one-hot encoding. The numerical features are rescaled to the same range (0 to 1) so that the learners can treat each feature equally. Different classification algorithms are compared. The performance of the models are measured by the F2 score in order to put more weights on recall. K fold cross validation procedure is implemented to evaluate the models. Random Forest classifier is chosen as it gives the best cross validation F2 score. The model is further tuned using &lt;code&gt;GridSearchCV&lt;/code&gt;. The final model shows excellent F2 score (much better than the naive benchmark classifier) and appears to be generalizable to unseen data. With this classification model, we can predict whether a current employee will leave given the features of the employee. Feature importance analysis reveals the top 5 features, which consist about 95% of the total importance. These top features can help management and human resource team understand why their emplpyees leave.&lt;/p&gt;

&lt;p&gt;However, the prediction results seem too good to be true in reality. But data leakage should not be a problem here since feature normalization is performed on each fold of cross validation instead of on the entire dataset. So, one possible explanation for this exceptionally good result could be that the synthesized dataset is just too ideal. The real-world data is normally a lot messier. Also, it is reasonable to believe that employee turnover characteristic would be different for each individual company, or at least for each type of industry. Therefore, this specific model obtained here is likely not suitable for specific companies out there. Nevertheless, the general machine learning approach described here should be a good practice in reality.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;[1] Cascio, W.F. 2006. Managing Human Resources: Productivity, Quality of Work Life, Profits (7th ed.). Burr Ridge, IL: Irwin/McGraw-Hill. Mitchell, T.R., Holtom, B.C., &amp;amp; Lee, T.W. 2001. How to keep your best employees&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>